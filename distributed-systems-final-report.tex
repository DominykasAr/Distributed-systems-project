\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{cleveref}

\newcommand{\emailaddr}[1]{\href{mailto:#1}{\texttt{#1}}}

\title{\LARGE
    Mini-Dynamo: A Distributed Key--Value Store in Python
}
\subtitle{Final Report for the Distributed Systems Course 2025/2026}

\author{Dominykas Arnauskas\\\emailaddr{dominykas.arnauskas@studio.unibo.it]}}

\date{}

\begin{document}
\maketitle

\section{Concept}\label{concept}

This project delivers a \textbf{distributed key--value store} (a software service) and supporting tooling for demonstration and evaluation.

\begin{itemize}
  \item \textbf{Product type:} a multi-node distributed storage service (application) plus a simple demo UI.
  \item \textbf{System goal:} accept client requests on any node and store/retrieve data reliably even when nodes fail.
  \item \textbf{Main functionalities:}
    \begin{itemize}
      \item \texttt{PUT(key, value)}: store/update a value for a key.
      \item \texttt{GET(key)}: retrieve the latest value for a key.
      \item \texttt{DELETE(key)}: delete a key using a tombstone record.
      \item Partitioning via consistent hashing (with virtual nodes).
      \item Replication to $R$ distinct nodes.
      \item Quorum reads/writes via parameters $W$ and $Q$.
      \item Membership and failure detection via heartbeats.
      \item Rebalancing (key handoff) on membership changes.
      \item Web UI for demo and observability (human-readable summaries + raw JSON).
    \end{itemize}
\end{itemize}

\section{Requirements Elicitation and Analysis}\label{requirements}

\textbf{Functional requirements}
\begin{itemize}
  \item Provide a client API for \texttt{PUT/GET/DELETE}.
  \item Allow clients to contact any node (coordinator-style handling).
  \item Partition keys across nodes using consistent hashing.
  \item Replicate records to multiple nodes and support quorum-based reads/writes.
  \item Detect failed peers and adapt routing to any available nodes.
  \item Support changes when new node joins or leaves without requiring a full restart.
  \item Provide a simple user interface for demonstration.
\end{itemize}

\textbf{Non-functional requirements}
\begin{itemize}
  \item Understandable implementation (clear modular structure; small components).
  \item Reproducible deployment (Docker + Docker Compose).
  \item Possibility for debugging (logging + a state endpoint).
  \item Unit tests for local logic, integration tests for multi-node behavior.
\end{itemize}

\subsection{Relevant Distributed System Features}
\begin{itemize}
  \item \textbf{Distribution and scalability:} partitioning via consistent hashing.
  \item \textbf{Fault tolerance:} replication and continued operation when a node fails.
  \item \textbf{Consistency trade-offs:} quorum reads/writes with tunable parameters $R/W/Q$.
  \item \textbf{Partial failures:} timeouts, node unavailability, and retry behavior.
  \item \textbf{Membership:} detection of node liveness via heartbeats.
  \item \textbf{Reconfiguration:} rebalancing data when nodes join or leave.
\end{itemize}

\section{Design}\label{design}

\subsection{Architecture}\label{architecture}
The system uses a \textbf{peer-to-peer cluster} where each node offers both public client endpoints and internal replica/membership endpoints. A client may send requests to any node; the receiving node acts as a \textbf{coordinator} for that request, computing the responsible replicas using the current hash ring and then contacting replicas to satisfy the requested quorum.

Nodes expose:
\begin{itemize}
  \item \textbf{Client API:} \texttt{/kv/put}, \texttt{/kv/get}, \texttt{/kv/delete}.
  \item \textbf{Internal replica API:} replica \texttt{put/get/delete} endpoints used for replication.
  \item \textbf{Debug/state API:} \texttt{/debug/state} for ring/membership visibility.
  \item \textbf{Membership API:} \texttt{/internal/heartbeat}.
\end{itemize}

\subsection{Infrastructure}\label{infrastructure}
Nodes run as separate processes (or containers) and communicate over HTTP. Deployment uses Docker Compose to launch multiple nodes and the UI service on a shared virtual network. Ports are mapped to the host for easy access.

\subsection{Modelling}\label{modelling}
\textbf{Record model.} Each key maps to a record containing:
\begin{itemize}
  \item \texttt{value}: optional string (absent for tombstones),
  \item \texttt{ts}: a floating-point timestamp,
  \item \texttt{tombstone}: boolean deletion marker.
\end{itemize}

\textbf{Conflict model.} The system uses \textbf{last-write-wins (LWW)}: the record with the greatest timestamp is considered true.

\subsection{Interaction}\label{interaction}
\textbf{Write path (PUT).} A coordinator determines the replica set for the key (size $R$) and sends internal replica writes to those nodes. The operation is acknowledged to the client once $W$ replicas confirm.
\textbf{Read path (GET).} A coordinator queries replicas and waits for $Q$ responses, then returns the newest record via LWW.
\textbf{Delete path (DELETE).} A coordinator replicates a tombstone record with a timestamp and waits for $W$ acknowledgments.

\subsection{Behaviour}\label{behaviour}
The system behaves correctly under the following normal and failure scenarios:
\begin{itemize}
  \item With no failures, reads and writes succeed and distribute load across nodes.
  \item If one node fails, operations still succeed.
  \item When a node joins or leaves, the membership changes update the ring; rebalancing transfers keys to new responsible replicas.
\end{itemize}

\subsection{Data and Consistency Issues}\label{data-and-consistency-issues}
Mini-Dynamo targets \textbf{eventual consistency}. With appropriate quorum choices, the probability of serving stale reads is reduced. Timestamps implement a simple conflict resolution scheme.
A key observation during development was that weak write quorums (e.g., $W=1$ with $R=2$) can lead to data loss under a single-node failure if the only acknowledged replica fails.

\subsection{Fault-Tolerance}\label{fault-tolerance}
Fault tolerance is achieved through replication and failure detection:
\begin{itemize}
  \item \textbf{Replication:} data is stored on $R$ distinct nodes.
  \item \textbf{Failure detection:} heartbeats mark peers as unavailable after a timeout.
  \item \textbf{Adaptive routing:} unavailable peers are excluded from the active ring.
  \item \textbf{Rebalancing:} when membership changes, nodes push locally stored keys toward the current replica set.
\end{itemize}

\subsection{Availability}\label{availability}
Availability depends on quorum configuration and the number of live replicas. The system can be configured to favor availability (small $W$ and $Q$) or stronger consistency/durability (larger $W$ and $Q$).

\subsection{Security}\label{security}
Security was not a primary focus for this prototype. The system assumes a trusted network environment (e.g., Docker Compose network) and does not implement authentication, authorization, TLS, or input hardening beyond basic validation. These could be added as future work.

\section{Implementation}\label{implementation}

\subsection{Technological details}\label{technological-details}
\begin{itemize}
  \item \textbf{Language:} Python.
  \item \textbf{Web framework:} FastAPI for node APIs and the demo UI.
  \item \textbf{Inter-node/client requests:} \texttt{httpx} (async HTTP client).
  \item \textbf{Concurrency:} \texttt{asyncio} for concurrent requests and background tasks (heartbeats).
  \item \textbf{Deployment:} Docker and Docker Compose.
  \item \textbf{Testing:} \texttt{pytest} for unit tests (hashing/store logic) and integration tests (multi-node behavior).
\end{itemize}

The codebase is organized into modules (hashing, membership, quorum logic, storage, node API, and UI) to keep the implementation navigable and to support easy debugging.

\section{Validation}\label{validation}

\subsection{Automatic Testing}\label{automatic-testing}
Automatic tests were implemented at two levels:
\begin{itemize}
  \item \textbf{Unit tests:} validate consistent hashing properties (stable ownership, unique replicas) and storage semantics (LWW conflict resolution, tombstones).
  \item \textbf{Integration tests:} spawn multiple node processes and verify end-to-end PUT/GET behavior, including operation success when a node is stopped (under suitable quorum settings).
\end{itemize}

\subsection{Acceptance test}\label{acceptance-test}
A manual acceptance test is performed using the web UI:
\begin{enumerate}
  \item Start a 3-node cluster (Docker Compose).
  \item Use the UI to PUT a key and then GET it from the cluster.
  \item Stop one node and repeat GET/PUT operations to demonstrate fault tolerance.
  \item Restart the node (or add/remove a node) and observe membership updates and rebalancing behavior via the node state panel.
\end{enumerate}

\section{Deployment}\label{deployment}
Deployment is provided via Docker and Docker Compose. A single Docker image is built for the project, and Compose runs multiple node services plus the UI service. This enables reproducible demonstrations on any machine with Docker installed.

\section{User Guide}\label{user-guide}
\textbf{Local run (without Docker).} Start three nodes in separate terminals using \texttt{python run\_node.py} with different ports and peer lists, then start the UI using \texttt{python run\_ui.py}.
\textbf{Docker run.} From the project root:
\begin{verbatim}
  docker compose up --build
\end{verbatim}
Open the UI at \texttt{http://127.0.0.1:9000}. The UI supports PUT/GET/DELETE and shows a simplified view of the node state (active ring, alive/dead peers, and $R/W/Q$ configuration).

\section{Self-evaluation}\label{self-evaluation}
This project met the main learning objectives: implementing a realistic data-partitioning scheme, adding replication, exploring quorum-based trade-offs, and handling partial failures via timeouts and membership updates. The most instructive aspect was observing how configuration choices (especially $W$) affect durability under failures and how membership changes require rebalancing to avoid apparent data loss.

\section{Future works}\label{future-works}
Possible extensions include:
\begin{itemize}
  \item \textbf{Persistence:} store records permanently (e.g., SQLite or append-only log) to survive full restarts.
  \item \textbf{Read repair:} after a read, push the newest record to stale replicas.
  \item \textbf{Hinted handoff:} temporarily store writes for unavailable nodes and forward them upon recovery.
  \item \textbf{Gossip membership:} replace centralized peer lists/heartbeats with gossip dissemination.
  \item \textbf{Metrics and monitoring:} request counters and latency histograms, plus a small dashboard.
  \item \textbf{Security:} authentication, TLS, and input hardening for untrusted networks.
\end{itemize}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
